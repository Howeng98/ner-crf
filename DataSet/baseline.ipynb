{"nbformat":4,"nbformat_minor":0,"metadata":{"file_extension":".py","kernelspec":{"display_name":"Python 3.7.3 64-bit ('base': conda)","language":"python","name":"python37364bitbaseconda25e194332beb4781976fd0b4d3e20954"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"mimetype":"text/x-python","name":"python","npconvert_exporter":"python","pygments_lexer":"ipython3","version":3,"colab":{"name":"baseline.ipynb","provenance":[],"toc_visible":true},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"xAUewAJ0RrnV","executionInfo":{"status":"ok","timestamp":1606200815912,"user_tz":-480,"elapsed":1263,"user":{"displayName":"Q56094027江昇翰","photoUrl":"","userId":"06927162653666667155"}}},"source":["import os\n","import sys\n","import numpy as np\n","import pandas as pd\n","from tqdm import tqdm"],"execution_count":24,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"V6LWkA3P8HZO","executionInfo":{"status":"ok","timestamp":1606200816847,"user_tz":-480,"elapsed":2190,"user":{"displayName":"Q56094027江昇翰","photoUrl":"","userId":"06927162653666667155"}},"outputId":"49777d3b-1e82-45b1-ad3e-a69b09208cf5"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":25,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"CloK6THIRrnV"},"source":["## Preprocessing\n","* Change input data (ex. train.txt) into CRF model input format (ex. train.data)\n","    * CRF model input format (ex. train.data):\n","        ```\n","        肝 O\n","        功 O\n","        能 O\n","        6 B-med_exam\n","        8 I-med_exam\n","        ```"]},{"cell_type":"code","metadata":{"id":"t1v91VGpRrnV","executionInfo":{"status":"ok","timestamp":1606200816848,"user_tz":-480,"elapsed":2189,"user":{"displayName":"Q56094027江昇翰","photoUrl":"","userId":"06927162653666667155"}}},"source":["file_path='/content/drive/MyDrive/AIMAS/Project3/SampleData_deid.txt'"],"execution_count":26,"outputs":[]},{"cell_type":"code","metadata":{"id":"KgtHEWd-RrnV","executionInfo":{"status":"ok","timestamp":1606200816848,"user_tz":-480,"elapsed":2187,"user":{"displayName":"Q56094027江昇翰","photoUrl":"","userId":"06927162653666667155"}}},"source":["def loadInputFile(path):\n","    trainingset = list()  # store trainingset [content,content,...]\n","    position = list()  # store position [article_id, start_pos, end_pos, entity_text, entity_type, ...]\n","    mentions = dict()  # store mentions[mention] = Type\n","    with open(file_path, 'r', encoding='utf8') as f:\n","        file_text=f.read().encode('utf-8').decode('utf-8-sig')\n","    datas=file_text.split('\\n\\n--------------------\\n\\n')[:-1]\n","    for data in datas:\n","        data=data.split('\\n')\n","        content=data[0]\n","        trainingset.append(content)\n","        annotations=data[1:]\n","        for annot in annotations[1:]:\n","            annot=annot.split('\\t') #annot= article_id, start_pos, end_pos, entity_text, entity_type\n","            position.extend(annot)\n","            mentions[annot[3]]=annot[4]\n","    \n","    return trainingset, position, mentions"],"execution_count":27,"outputs":[]},{"cell_type":"code","metadata":{"id":"5avpn8FTRrnV","executionInfo":{"status":"ok","timestamp":1606200816849,"user_tz":-480,"elapsed":2187,"user":{"displayName":"Q56094027江昇翰","photoUrl":"","userId":"06927162653666667155"}}},"source":["def CRFFormatData(trainingset, position, path):\n","    if (os.path.isfile(path)):\n","        os.remove(path)\n","    outputfile = open(path, 'a', encoding= 'utf-8')\n","\n","    # output file lines\n","    count = 0 # annotation counts in each content\n","    tagged = list()\n","    for article_id in range(len(trainingset)):\n","        trainingset_split = list(trainingset[article_id])\n","        while '' or ' ' in trainingset_split:\n","            if '' in trainingset_split:\n","                trainingset_split.remove('')\n","            else:\n","                trainingset_split.remove(' ')\n","        start_tmp = 0\n","        for position_idx in range(0,len(position),5):\n","            if int(position[position_idx]) == article_id:\n","                count += 1\n","                if count == 1:\n","                    start_pos = int(position[position_idx+1])\n","                    end_pos = int(position[position_idx+2])\n","                    entity_type=position[position_idx+4]\n","                    if start_pos == 0:\n","                        token = list(trainingset[article_id][start_pos:end_pos])\n","                        whole_token = trainingset[article_id][start_pos:end_pos]\n","                        for token_idx in range(len(token)):\n","                            if len(token[token_idx].replace(' ','')) == 0:\n","                                continue\n","                            # BIO states\n","                            if token_idx == 0:\n","                                label = 'B-'+entity_type\n","                            else:\n","                                label = 'I-'+entity_type\n","                            \n","                            output_str = token[token_idx] + ' ' + label + '\\n'\n","                            outputfile.write(output_str)\n","\n","                    else:\n","                        token = list(trainingset[article_id][0:start_pos])\n","                        whole_token = trainingset[article_id][0:start_pos]\n","                        for token_idx in range(len(token)):\n","                            if len(token[token_idx].replace(' ','')) == 0:\n","                                continue\n","                            \n","                            output_str = token[token_idx] + ' ' + 'O' + '\\n'\n","                            outputfile.write(output_str)\n","\n","                        token = list(trainingset[article_id][start_pos:end_pos])\n","                        whole_token = trainingset[article_id][start_pos:end_pos]\n","                        for token_idx in range(len(token)):\n","                            if len(token[token_idx].replace(' ','')) == 0:\n","                                continue\n","                            # BIO states\n","                            if token[0] == '':\n","                                if token_idx == 1:\n","                                    label = 'B-'+entity_type\n","                                else:\n","                                    label = 'I-'+entity_type\n","                            else:\n","                                if token_idx == 0:\n","                                    label = 'B-'+entity_type\n","                                else:\n","                                    label = 'I-'+entity_type\n","\n","                            output_str = token[token_idx] + ' ' + label + '\\n'\n","                            outputfile.write(output_str)\n","\n","                    start_tmp = end_pos\n","                else:\n","                    start_pos = int(position[position_idx+1])\n","                    end_pos = int(position[position_idx+2])\n","                    entity_type=position[position_idx+4]\n","                    if start_pos<start_tmp:\n","                        continue\n","                    else:\n","                        token = list(trainingset[article_id][start_tmp:start_pos])\n","                        whole_token = trainingset[article_id][start_tmp:start_pos]\n","                        for token_idx in range(len(token)):\n","                            if len(token[token_idx].replace(' ','')) == 0:\n","                                continue\n","                            output_str = token[token_idx] + ' ' + 'O' + '\\n'\n","                            outputfile.write(output_str)\n","\n","                    token = list(trainingset[article_id][start_pos:end_pos])\n","                    whole_token = trainingset[article_id][start_pos:end_pos]\n","                    for token_idx in range(len(token)):\n","                        if len(token[token_idx].replace(' ','')) == 0:\n","                            continue\n","                        # BIO states\n","                        if token[0] == '':\n","                            if token_idx == 1:\n","                                label = 'B-'+entity_type\n","                            else:\n","                                label = 'I-'+entity_type\n","                        else:\n","                            if token_idx == 0:\n","                                label = 'B-'+entity_type\n","                            else:\n","                                label = 'I-'+entity_type\n","                        \n","                        output_str = token[token_idx] + ' ' + label + '\\n'\n","                        outputfile.write(output_str)\n","                    start_tmp = end_pos\n","\n","        token = list(trainingset[article_id][start_tmp:])\n","        whole_token = trainingset[article_id][start_tmp:]\n","        for token_idx in range(len(token)):\n","            if len(token[token_idx].replace(' ','')) == 0:\n","                continue\n","\n","            \n","            output_str = token[token_idx] + ' ' + 'O' + '\\n'\n","            outputfile.write(output_str)\n","\n","        count = 0\n","    \n","        output_str = '\\n'\n","        outputfile.write(output_str)\n","        ID = trainingset[article_id]\n","\n","        if article_id%10 == 0:\n","            print('Total complete articles:', article_id)\n","\n","    # close output file\n","    outputfile.close()"],"execution_count":28,"outputs":[]},{"cell_type":"code","metadata":{"id":"G_4dYniVRrnV","executionInfo":{"status":"ok","timestamp":1606200816850,"user_tz":-480,"elapsed":2186,"user":{"displayName":"Q56094027江昇翰","photoUrl":"","userId":"06927162653666667155"}}},"source":["trainingset, position, mentions=loadInputFile(file_path)"],"execution_count":29,"outputs":[]},{"cell_type":"code","metadata":{"id":"Nk4hGQu1RrnV","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606200816851,"user_tz":-480,"elapsed":2182,"user":{"displayName":"Q56094027江昇翰","photoUrl":"","userId":"06927162653666667155"}},"outputId":"ac64b89f-d8e6-4d6f-960b-af99ca9ec79e"},"source":["data_path='data/sample.data'\n","CRFFormatData(trainingset, position, data_path)"],"execution_count":30,"outputs":[{"output_type":"stream","text":["Total complete articles: 0\n","Total complete articles: 10\n","Total complete articles: 20\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"zuYBIWnlRrnW"},"source":["## NER model\n","### CRF (Conditional Random Field model)\n","* Using `sklearn-crfsuite` API\n","\n","    (you may try `CRF++`, `python-crfsuite`, `pytorch-crfsuite`(neural network version))"]},{"cell_type":"code","metadata":{"id":"mI9UJRfKRrnW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606200819269,"user_tz":-480,"elapsed":4595,"user":{"displayName":"Q56094027江昇翰","photoUrl":"","userId":"06927162653666667155"}},"outputId":"e8d26d0b-d487-4fa8-b6c4-e4e642a7d315"},"source":["!pip install sklearn-crfsuite\n","import sklearn_crfsuite\n","\n","from sklearn_crfsuite import scorers\n","from sklearn_crfsuite import metrics\n","from sklearn_crfsuite.metrics import flat_classification_report"],"execution_count":31,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: sklearn-crfsuite in /usr/local/lib/python3.6/dist-packages (0.3.6)\n","Requirement already satisfied: tabulate in /usr/local/lib/python3.6/dist-packages (from sklearn-crfsuite) (0.8.7)\n","Requirement already satisfied: tqdm>=2.0 in /usr/local/lib/python3.6/dist-packages (from sklearn-crfsuite) (4.41.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sklearn-crfsuite) (1.15.0)\n","Requirement already satisfied: python-crfsuite>=0.8.3 in /usr/local/lib/python3.6/dist-packages (from sklearn-crfsuite) (0.9.7)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"6GMS2hCoRrnW","executionInfo":{"status":"ok","timestamp":1606200819270,"user_tz":-480,"elapsed":4594,"user":{"displayName":"Q56094027江昇翰","photoUrl":"","userId":"06927162653666667155"}}},"source":["def CRF(x_train, y_train, x_test, y_test):\n","    crf = sklearn_crfsuite.CRF(\n","        algorithm='lbfgs',\n","        c1=0.1,\n","        c2=0.1,\n","        max_iterations=100,\n","        all_possible_transitions=True\n","    )\n","    crf.fit(x_train, y_train)\n","    # print(crf)\n","    y_pred = crf.predict(x_test)\n","    y_pred_mar = crf.predict_marginals(x_test)\n","\n","    # print(y_pred_mar)\n","\n","    labels = list(crf.classes_)\n","    labels.remove('O')\n","    f1score = metrics.flat_f1_score(y_test, y_pred, average='weighted', labels=labels)\n","    sorted_labels = sorted(labels,key=lambda name: (name[1:], name[0])) # group B and I results\n","    print(flat_classification_report(y_test, y_pred, labels=sorted_labels, digits=3))\n","    return y_pred, y_pred_mar, f1score"],"execution_count":32,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Qhx6pJSHRrnW"},"source":["## Model Input: \n","* input features:\n","    * word vector: pretrained traditional chinese word embedding by Word2Vec-CBOW\n","    \n","    (you may try add some other features, ex. pos-tag, word_length, word_position, ...) "]},{"cell_type":"code","metadata":{"id":"7UculQjKRrnW","executionInfo":{"status":"ok","timestamp":1606200819271,"user_tz":-480,"elapsed":4594,"user":{"displayName":"Q56094027江昇翰","photoUrl":"","userId":"06927162653666667155"}}},"source":["import numpy as np"],"execution_count":33,"outputs":[]},{"cell_type":"code","metadata":{"id":"tHtTSi_8RrnW","executionInfo":{"status":"ok","timestamp":1606200844363,"user_tz":-480,"elapsed":29684,"user":{"displayName":"Q56094027江昇翰","photoUrl":"","userId":"06927162653666667155"}}},"source":["# load pretrained word vectors\n","# get a dict of tokens (key) and their pretrained word vectors (value)\n","# pretrained word2vec CBOW word vector: https://fgc.stpi.narl.org.tw/activity/videoDetail/4b1141305ddf5522015de5479f4701b1\n","dim = 0\n","word_vecs= {}\n","# open pretrained word vector file\n","with open('/content/drive/MyDrive/AIMAS/Project3/cna.cbow.cwe_p.tar_g.512d.0.txt') as f:\n","    for line in f:\n","        tokens = line.strip().split()\n","\n","        # there 2 integers in the first line: vocabulary_size, word_vector_dim\n","        if len(tokens) == 2:\n","            dim = int(tokens[1])\n","            continue\n","    \n","        word = tokens[0] \n","        vec = np.array([ float(t) for t in tokens[1:] ])\n","        word_vecs[word] = vec"],"execution_count":34,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"id":"ey0U_XhpRrnW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606200844366,"user_tz":-480,"elapsed":29683,"user":{"displayName":"Q56094027江昇翰","photoUrl":"","userId":"06927162653666667155"}},"outputId":"cb6a5b03-8f81-4cf3-d691-03c58bf7ce45"},"source":["print('vocabulary_size: ',len(word_vecs),' word_vector_dim: ',vec.shape)"],"execution_count":35,"outputs":[{"output_type":"stream","text":["vocabulary_size:  158566  word_vector_dim:  (512,)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"W6RsMn2QRrnW"},"source":["Here we split data into training dataset and testing dataset,\n","however, we'll provide `development data` and `test data` which is real testing dataset.\n","\n","You should upload prediction on `development data` and `test data` to system, not this splitted testing dataset."]},{"cell_type":"code","metadata":{"id":"ySlER2ErRrnW","executionInfo":{"status":"ok","timestamp":1606200844367,"user_tz":-480,"elapsed":29682,"user":{"displayName":"Q56094027江昇翰","photoUrl":"","userId":"06927162653666667155"}}},"source":["# load `train.data` and separate into a list of labeled data of each text\n","# return:\n","#   data_list: a list of lists of tuples, storing tokens and labels (wrapped in tuple) of each text in `train.data`\n","#   traindata_list: a list of lists, storing training data_list splitted from data_list\n","#   testdata_list: a list of lists, storing testing data_list splitted from data_list\n","from sklearn.model_selection import train_test_split\n","def Dataset(data_path):\n","    with open(data_path, 'r', encoding='utf-8') as f:\n","        data=f.readlines()#.encode('utf-8').decode('utf-8-sig')\n","    data_list, data_list_tmp = list(), list()\n","    article_id_list=list()\n","    idx=0\n","    for row in data:\n","        data_tuple = tuple()\n","        if row == '\\n':\n","            article_id_list.append(idx)\n","            idx+=1\n","            data_list.append(data_list_tmp)\n","            data_list_tmp = []\n","        else:\n","            row = row.strip('\\n').split(' ')\n","            data_tuple = (row[0], row[1])\n","            data_list_tmp.append(data_tuple)\n","    if len(data_list_tmp) != 0:\n","        data_list.append(data_list_tmp)\n","    \n","    # here we random split data into training dataset and testing dataset\n","    # but you should take `development data` or `test data` as testing data\n","    # At that time, you could just delete this line, \n","    # and generate data_list of `train data` and data_list of `development/test data` by this function\n","    traindata_list, testdata_list, traindata_article_id_list, testdata_article_id_list=train_test_split(data_list,\n","                                                                                                    article_id_list,\n","                                                                                                    test_size=0.33,\n","                                                                                                    random_state=42)\n","    \n","    return data_list, traindata_list, testdata_list, traindata_article_id_list, testdata_article_id_list "],"execution_count":36,"outputs":[]},{"cell_type":"code","metadata":{"id":"T3P8vbYGRrnX","executionInfo":{"status":"ok","timestamp":1606200844367,"user_tz":-480,"elapsed":29680,"user":{"displayName":"Q56094027江昇翰","photoUrl":"","userId":"06927162653666667155"}}},"source":["# look up word vectors\n","# turn each word into its pretrained word vector\n","# return a list of word vectors corresponding to each token in train.data\n","def Word2Vector(data_list, embedding_dict):\n","    embedding_list = list()\n","\n","    # No Match Word (unknown word) Vector in Embedding\n","    unk_vector=np.random.rand(*(list(embedding_dict.values())[0].shape))\n","\n","    for idx_list in range(len(data_list)):\n","        embedding_list_tmp = list()\n","        for idx_tuple in range(len(data_list[idx_list])):\n","            key = data_list[idx_list][idx_tuple][0] # token\n","\n","            if key in embedding_dict:\n","                value = embedding_dict[key]\n","            else:\n","                value = unk_vector\n","            embedding_list_tmp.append(value)\n","        embedding_list.append(embedding_list_tmp)\n","    return embedding_list"],"execution_count":37,"outputs":[]},{"cell_type":"code","metadata":{"id":"qzmKa42yRrnX","executionInfo":{"status":"ok","timestamp":1606200844368,"user_tz":-480,"elapsed":29679,"user":{"displayName":"Q56094027江昇翰","photoUrl":"","userId":"06927162653666667155"}}},"source":["# input features: pretrained word vectors of each token\n","# return a list of feature dicts, each feature dict corresponding to each token\n","def Feature(embed_list):\n","    feature_list = list()\n","    for idx_list in range(len(embed_list)):\n","        feature_list_tmp = list()\n","        for idx_tuple in range(len(embed_list[idx_list])):\n","            feature_dict = dict()\n","            for idx_vec in range(len(embed_list[idx_list][idx_tuple])):\n","                feature_dict['dim_' + str(idx_vec+1)] = embed_list[idx_list][idx_tuple][idx_vec]\n","            feature_list_tmp.append(feature_dict)\n","        feature_list.append(feature_list_tmp)\n","    return feature_list"],"execution_count":38,"outputs":[]},{"cell_type":"code","metadata":{"id":"dNa4fSkYRrnX","executionInfo":{"status":"ok","timestamp":1606200844368,"user_tz":-480,"elapsed":29678,"user":{"displayName":"Q56094027江昇翰","photoUrl":"","userId":"06927162653666667155"}}},"source":["# get the labels of each tokens in train.data\n","# return a list of lists of labels\n","def Preprocess(data_list):\n","    label_list = list()\n","    for idx_list in range(len(data_list)):\n","        label_list_tmp = list()\n","        for idx_tuple in range(len(data_list[idx_list])):\n","            label_list_tmp.append(data_list[idx_list][idx_tuple][1])\n","        label_list.append(label_list_tmp)\n","    return label_list"],"execution_count":39,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Wdoz84b0RrnX"},"source":["## Training"]},{"cell_type":"code","metadata":{"id":"e_uGdFy3RrnX","executionInfo":{"status":"ok","timestamp":1606200844369,"user_tz":-480,"elapsed":29675,"user":{"displayName":"Q56094027江昇翰","photoUrl":"","userId":"06927162653666667155"}}},"source":["data_list, traindata_list, testdata_list, traindata_article_id_list, testdata_article_id_list = Dataset(data_path)"],"execution_count":40,"outputs":[]},{"cell_type":"code","metadata":{"id":"BDc_mtuXRrnX","executionInfo":{"status":"ok","timestamp":1606200857085,"user_tz":-480,"elapsed":42390,"user":{"displayName":"Q56094027江昇翰","photoUrl":"","userId":"06927162653666667155"}}},"source":["# Load Word Embedding\n","trainembed_list = Word2Vector(traindata_list, word_vecs)\n","testembed_list = Word2Vector(testdata_list, word_vecs)\n","\n","# CRF - Train Data (Augmentation Data)\n","x_train = Feature(trainembed_list)\n","y_train = Preprocess(traindata_list)\n","\n","# CRF - Test Data (Golden Standard)\n","x_test = Feature(testembed_list)\n","y_test = Preprocess(testdata_list)"],"execution_count":41,"outputs":[]},{"cell_type":"code","metadata":{"id":"hbhes9eBRrnX"},"source":["y_pred, y_pred_mar, f1score = CRF(x_train, y_train, x_test, y_test)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"id":"H02UitQBRrnX"},"source":["f1score"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"j0nOItHSRrnX"},"source":["## Output data\n","* Change model output into `output.tsv` \n","* Only accept this output format uploading to competition system"]},{"cell_type":"code","metadata":{"id":"QC66RB5tRrnX"},"source":["output=\"article_id\\tstart_position\\tend_position\\tentity_text\\tentity_type\\n\"\n","for test_id in range(len(y_pred)):\n","    pos=0\n","    start_pos=None\n","    end_pos=None\n","    entity_text=None\n","    entity_type=None\n","    for pred_id in range(len(y_pred[test_id])):\n","        if y_pred[test_id][pred_id][0]=='B':\n","            start_pos=pos\n","            entity_type=y_pred[test_id][pred_id][2:]\n","        elif start_pos is not None and y_pred[test_id][pred_id][0]=='I' and y_pred[test_id][pred_id+1][0]=='O':\n","            end_pos=pos\n","            entity_text=''.join([testdata_list[test_id][position][0] for position in range(start_pos,end_pos+1)])\n","            line=str(testdata_article_id_list[test_id])+'\\t'+str(start_pos)+'\\t'+str(end_pos+1)+'\\t'+entity_text+'\\t'+entity_type\n","            output+=line+'\\n'\n","        pos+=1     "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"40HnTWGtRrnX"},"source":["output_path='output.tsv'\n","with open(output_path,'w',encoding='utf-8') as f:\n","    f.write(output)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"id":"-M36pQhxRrnX"},"source":["print(output)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"36oJuO06RrnX"},"source":["## Note\n","* You may try `python-crfsuite` to train an neural network for NER tagging optimized by gradient descent back propagation\n","    * [Documentation](https://github.com/scrapinghub/python-crfsuite)\n","* You may try `CRF++` tool for NER tagging by CRF model\n","    * [Documentation](http://taku910.github.io/crfpp/)\n","    * Need design feature template\n","    * Can only computed in CPU\n","* You may try other traditional chinese word embedding (ex. fasttext, bert, ...) for input features\n","* You may try add other features for NER model, ex. POS-tag, word_length, word_position, ...\n","* You should upload the prediction output on `development data` or `test data` provided later to the competition system. Note don't upload prediction output on the splitted testing dataset like this baseline example."]},{"cell_type":"markdown","metadata":{"id":"yOuPLXj8RrnX"},"source":["-----------------------------------------------------"]}]}