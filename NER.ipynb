{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "file_extension": ".py",
    "kernelspec": {
      "display_name": "Python 3.7.3 64-bit ('base': conda)",
      "language": "python",
      "name": "python37364bitbaseconda25e194332beb4781976fd0b4d3e20954"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "mimetype": "text/x-python",
    "name": "python",
    "npconvert_exporter": "python",
    "pygments_lexer": "ipython3",
    "version": 3,
    "colab": {
      "name": "NER.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "xAUewAJ0RrnV"
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V6LWkA3P8HZO",
        "outputId": "02548c87-5767-4a2e-d561-973d25aff73a"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CloK6THIRrnV"
      },
      "source": [
        "## Preprocessing\n",
        "* Change input data (ex. train.txt) into CRF model input format (ex. train.data)\n",
        "    * CRF model input format (ex. train.data):\n",
        "        ```\n",
        "        肝 O\n",
        "        功 O\n",
        "        能 O\n",
        "        6 B-med_exam\n",
        "        8 I-med_exam\n",
        "        ```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t1v91VGpRrnV"
      },
      "source": [
        "file_path='/content/drive/MyDrive/Colab Notebooks/NER/SampleData_deid.txt'"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KgtHEWd-RrnV"
      },
      "source": [
        "def loadInputFile(path):\n",
        "    trainingset = list()  # store trainingset [content,content,...]\n",
        "    position = list()  # store position [article_id, start_pos, end_pos, entity_text, entity_type, ...]\n",
        "    mentions = dict()  # store mentions[mention] = Type\n",
        "    with open(file_path, 'r', encoding='utf8') as f:\n",
        "        file_text=f.read().encode('utf-8').decode('utf-8-sig')\n",
        "    datas=file_text.split('\\n\\n--------------------\\n\\n')[:-1]\n",
        "    for data in datas:\n",
        "        data=data.split('\\n')\n",
        "        content=data[0]\n",
        "        trainingset.append(content)\n",
        "        annotations=data[1:]\n",
        "        for annot in annotations[1:]:\n",
        "            annot=annot.split('\\t') #annot= article_id, start_pos, end_pos, entity_text, entity_type\n",
        "            position.extend(annot)\n",
        "            mentions[annot[3]]=annot[4]\n",
        "    \n",
        "    return trainingset, position, mentions"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5avpn8FTRrnV"
      },
      "source": [
        "def CRFFormatData(trainingset, position, path):\n",
        "    if (os.path.isfile(path)):\n",
        "        os.remove(path)\n",
        "    outputfile = open(path, 'a', encoding= 'utf-8')\n",
        "\n",
        "    # output file lines\n",
        "    count = 0 # annotation counts in each content\n",
        "    tagged = list()\n",
        "    for article_id in range(len(trainingset)):\n",
        "        trainingset_split = list(trainingset[article_id])\n",
        "        while '' or ' ' in trainingset_split:\n",
        "            if '' in trainingset_split:\n",
        "                trainingset_split.remove('')\n",
        "            else:\n",
        "                trainingset_split.remove(' ')\n",
        "        start_tmp = 0\n",
        "        for position_idx in range(0,len(position),5):\n",
        "            if int(position[position_idx]) == article_id:\n",
        "                count += 1\n",
        "                if count == 1:\n",
        "                    start_pos = int(position[position_idx+1])\n",
        "                    end_pos = int(position[position_idx+2])\n",
        "                    entity_type=position[position_idx+4]\n",
        "                    if start_pos == 0:\n",
        "                        token = list(trainingset[article_id][start_pos:end_pos])\n",
        "                        whole_token = trainingset[article_id][start_pos:end_pos]\n",
        "                        for token_idx in range(len(token)):\n",
        "                            if len(token[token_idx].replace(' ','')) == 0:\n",
        "                                continue\n",
        "                            # BIO states\n",
        "                            if token_idx == 0:\n",
        "                                label = 'B-'+entity_type\n",
        "                            else:\n",
        "                                label = 'I-'+entity_type\n",
        "                            \n",
        "                            output_str = token[token_idx] + ' ' + label + '\\n'\n",
        "                            outputfile.write(output_str)\n",
        "\n",
        "                    else:\n",
        "                        token = list(trainingset[article_id][0:start_pos])\n",
        "                        whole_token = trainingset[article_id][0:start_pos]\n",
        "                        for token_idx in range(len(token)):\n",
        "                            if len(token[token_idx].replace(' ','')) == 0:\n",
        "                                continue\n",
        "                            \n",
        "                            output_str = token[token_idx] + ' ' + 'O' + '\\n'\n",
        "                            outputfile.write(output_str)\n",
        "\n",
        "                        token = list(trainingset[article_id][start_pos:end_pos])\n",
        "                        whole_token = trainingset[article_id][start_pos:end_pos]\n",
        "                        for token_idx in range(len(token)):\n",
        "                            if len(token[token_idx].replace(' ','')) == 0:\n",
        "                                continue\n",
        "                            # BIO states\n",
        "                            if token[0] == '':\n",
        "                                if token_idx == 1:\n",
        "                                    label = 'B-'+entity_type\n",
        "                                else:\n",
        "                                    label = 'I-'+entity_type\n",
        "                            else:\n",
        "                                if token_idx == 0:\n",
        "                                    label = 'B-'+entity_type\n",
        "                                else:\n",
        "                                    label = 'I-'+entity_type\n",
        "\n",
        "                            output_str = token[token_idx] + ' ' + label + '\\n'\n",
        "                            outputfile.write(output_str)\n",
        "\n",
        "                    start_tmp = end_pos\n",
        "                else:\n",
        "                    start_pos = int(position[position_idx+1])\n",
        "                    end_pos = int(position[position_idx+2])\n",
        "                    entity_type=position[position_idx+4]\n",
        "                    if start_pos<start_tmp:\n",
        "                        continue\n",
        "                    else:\n",
        "                        token = list(trainingset[article_id][start_tmp:start_pos])\n",
        "                        whole_token = trainingset[article_id][start_tmp:start_pos]\n",
        "                        for token_idx in range(len(token)):\n",
        "                            if len(token[token_idx].replace(' ','')) == 0:\n",
        "                                continue\n",
        "                            output_str = token[token_idx] + ' ' + 'O' + '\\n'\n",
        "                            outputfile.write(output_str)\n",
        "\n",
        "                    token = list(trainingset[article_id][start_pos:end_pos])\n",
        "                    whole_token = trainingset[article_id][start_pos:end_pos]\n",
        "                    for token_idx in range(len(token)):\n",
        "                        if len(token[token_idx].replace(' ','')) == 0:\n",
        "                            continue\n",
        "                        # BIO states\n",
        "                        if token[0] == '':\n",
        "                            if token_idx == 1:\n",
        "                                label = 'B-'+entity_type\n",
        "                            else:\n",
        "                                label = 'I-'+entity_type\n",
        "                        else:\n",
        "                            if token_idx == 0:\n",
        "                                label = 'B-'+entity_type\n",
        "                            else:\n",
        "                                label = 'I-'+entity_type\n",
        "                        \n",
        "                        output_str = token[token_idx] + ' ' + label + '\\n'\n",
        "                        outputfile.write(output_str)\n",
        "                    start_tmp = end_pos\n",
        "\n",
        "        token = list(trainingset[article_id][start_tmp:])\n",
        "        whole_token = trainingset[article_id][start_tmp:]\n",
        "        for token_idx in range(len(token)):\n",
        "            if len(token[token_idx].replace(' ','')) == 0:\n",
        "                continue\n",
        "\n",
        "            \n",
        "            output_str = token[token_idx] + ' ' + 'O' + '\\n'\n",
        "            outputfile.write(output_str)\n",
        "\n",
        "        count = 0\n",
        "    \n",
        "        output_str = '\\n'\n",
        "        outputfile.write(output_str)\n",
        "        ID = trainingset[article_id]\n",
        "\n",
        "        if article_id%10 == 0:\n",
        "            print('Total complete articles:', article_id)\n",
        "\n",
        "    # close output file\n",
        "    outputfile.close()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G_4dYniVRrnV"
      },
      "source": [
        "trainingset, position, mentions=loadInputFile(file_path)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nk4hGQu1RrnV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d531bf8-1312-4a62-bed2-274c122893fe"
      },
      "source": [
        "data_path='/content/drive/MyDrive/Colab Notebooks/NER/sample.data'\n",
        "CRFFormatData(trainingset, position, data_path)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total complete articles: 0\n",
            "Total complete articles: 10\n",
            "Total complete articles: 20\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zuYBIWnlRrnW"
      },
      "source": [
        "## NER model\n",
        "### CRF (Conditional Random Field model)\n",
        "* Using `sklearn-crfsuite` API\n",
        "\n",
        "    (you may try `CRF++`, `python-crfsuite`, `pytorch-crfsuite`(neural network version))"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mI9UJRfKRrnW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f172471-7fc3-4961-db19-c3c416d94547"
      },
      "source": [
        "!pip install sklearn-crfsuite\n",
        "import sklearn_crfsuite\n",
        "\n",
        "from sklearn_crfsuite import scorers\n",
        "from sklearn_crfsuite import metrics\n",
        "from sklearn_crfsuite.metrics import flat_classification_report"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sklearn-crfsuite\n",
            "  Downloading https://files.pythonhosted.org/packages/25/74/5b7befa513482e6dee1f3dd68171a6c9dfc14c0eaa00f885ffeba54fe9b0/sklearn_crfsuite-0.3.6-py2.py3-none-any.whl\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.6/dist-packages (from sklearn-crfsuite) (0.8.7)\n",
            "Requirement already satisfied: tqdm>=2.0 in /usr/local/lib/python3.6/dist-packages (from sklearn-crfsuite) (4.41.1)\n",
            "Collecting python-crfsuite>=0.8.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/95/99/869dde6dbf3e0d07a013c8eebfb0a3d30776334e0097f8432b631a9a3a19/python_crfsuite-0.9.7-cp36-cp36m-manylinux1_x86_64.whl (743kB)\n",
            "\u001b[K     |████████████████████████████████| 747kB 5.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sklearn-crfsuite) (1.15.0)\n",
            "Installing collected packages: python-crfsuite, sklearn-crfsuite\n",
            "Successfully installed python-crfsuite-0.9.7 sklearn-crfsuite-0.3.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6GMS2hCoRrnW"
      },
      "source": [
        "def CRF(x_train, y_train, x_test, y_test):\n",
        "    crf = sklearn_crfsuite.CRF(\n",
        "        algorithm='lbfgs',\n",
        "        c1=0.1,\n",
        "        c2=0.1,\n",
        "        max_iterations=100,\n",
        "        all_possible_transitions=True\n",
        "    )\n",
        "    crf.fit(x_train, y_train)\n",
        "    # print(crf)\n",
        "    y_pred = crf.predict(x_test)\n",
        "    y_pred_mar = crf.predict_marginals(x_test)\n",
        "\n",
        "    # print(y_pred_mar)\n",
        "\n",
        "    labels = list(crf.classes_)\n",
        "    labels.remove('O')\n",
        "    f1score = metrics.flat_f1_score(y_test, y_pred, average='weighted', labels=labels)\n",
        "    sorted_labels = sorted(labels,key=lambda name: (name[1:], name[0])) # group B and I results\n",
        "    print(flat_classification_report(y_test, y_pred, labels=sorted_labels, digits=3))\n",
        "    return y_pred, y_pred_mar, f1score"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qhx6pJSHRrnW"
      },
      "source": [
        "## Model Input: \n",
        "* input features:\n",
        "    * word vector: pretrained traditional chinese word embedding by Word2Vec-CBOW\n",
        "    \n",
        "    (you may try add some other features, ex. pos-tag, word_length, word_position, ...) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7UculQjKRrnW"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tHtTSi_8RrnW"
      },
      "source": [
        "# load pretrained word vectors\n",
        "# get a dict of tokens (key) and their pretrained word vectors (value)\n",
        "# pretrained word2vec CBOW word vector: https://fgc.stpi.narl.org.tw/activity/videoDetail/4b1141305ddf5522015de5479f4701b1\n",
        "dim = 0\n",
        "word_vecs= {}\n",
        "# open pretrained word vector file\n",
        "with open('/content/drive/MyDrive/Colab Notebooks/NER/cna.cbow.cwe_p.tar_g.512d.0.txt') as f:\n",
        "    for line in f:\n",
        "        tokens = line.strip().split()\n",
        "\n",
        "        # there 2 integers in the first line: vocabulary_size, word_vector_dim\n",
        "        if len(tokens) == 2:\n",
        "            dim = int(tokens[1])\n",
        "            continue\n",
        "    \n",
        "        word = tokens[0] \n",
        "        vec = np.array([ float(t) for t in tokens[1:] ])\n",
        "        word_vecs[word] = vec"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "ey0U_XhpRrnW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53929406-b330-40c8-909f-1be509a0cb69"
      },
      "source": [
        "print('vocabulary_size: ',len(word_vecs),' word_vector_dim: ',vec.shape)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "vocabulary_size:  158566  word_vector_dim:  (512,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W6RsMn2QRrnW"
      },
      "source": [
        "Here we split data into training dataset and testing dataset,\n",
        "however, we'll provide `development data` and `test data` which is real testing dataset.\n",
        "\n",
        "You should upload prediction on `development data` and `test data` to system, not this splitted testing dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ySlER2ErRrnW"
      },
      "source": [
        "# load `train.data` and separate into a list of labeled data of each text\n",
        "# return:\n",
        "#   data_list: a list of lists of tuples, storing tokens and labels (wrapped in tuple) of each text in `train.data`\n",
        "#   traindata_list: a list of lists, storing training data_list splitted from data_list\n",
        "#   testdata_list: a list of lists, storing testing data_list splitted from data_list\n",
        "from sklearn.model_selection import train_test_split\n",
        "def Dataset(data_path):\n",
        "    with open(data_path, 'r', encoding='utf-8') as f:\n",
        "        data=f.readlines()#.encode('utf-8').decode('utf-8-sig')\n",
        "    data_list, data_list_tmp = list(), list()\n",
        "    article_id_list=list()\n",
        "    idx=0\n",
        "    for row in data:\n",
        "        data_tuple = tuple()\n",
        "        if row == '\\n':\n",
        "            article_id_list.append(idx)\n",
        "            idx+=1\n",
        "            data_list.append(data_list_tmp)\n",
        "            data_list_tmp = []\n",
        "        else:\n",
        "            row = row.strip('\\n').split(' ')\n",
        "            data_tuple = (row[0], row[1])\n",
        "            data_list_tmp.append(data_tuple)\n",
        "    if len(data_list_tmp) != 0:\n",
        "        data_list.append(data_list_tmp)\n",
        "    \n",
        "    # here we random split data into training dataset and testing dataset\n",
        "    # but you should take `development data` or `test data` as testing data\n",
        "    # At that time, you could just delete this line, \n",
        "    # and generate data_list of `train data` and data_list of `development/test data` by this function\n",
        "    traindata_list, testdata_list, traindata_article_id_list, testdata_article_id_list=train_test_split(data_list,\n",
        "                                                                                                    article_id_list,\n",
        "                                                                                                    test_size=0.33,\n",
        "                                                                                                    random_state=42)\n",
        "    \n",
        "    return data_list, traindata_list, testdata_list, traindata_article_id_list, testdata_article_id_list "
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T3P8vbYGRrnX"
      },
      "source": [
        "# look up word vectors\n",
        "# turn each word into its pretrained word vector\n",
        "# return a list of word vectors corresponding to each token in train.data\n",
        "def Word2Vector(data_list, embedding_dict):\n",
        "    embedding_list = list()\n",
        "\n",
        "    # No Match Word (unknown word) Vector in Embedding\n",
        "    unk_vector=np.random.rand(*(list(embedding_dict.values())[0].shape))\n",
        "\n",
        "    for idx_list in range(len(data_list)):\n",
        "        embedding_list_tmp = list()\n",
        "        for idx_tuple in range(len(data_list[idx_list])):\n",
        "            key = data_list[idx_list][idx_tuple][0] # token\n",
        "\n",
        "            if key in embedding_dict:\n",
        "                value = embedding_dict[key]\n",
        "            else:\n",
        "                value = unk_vector\n",
        "            embedding_list_tmp.append(value)\n",
        "        embedding_list.append(embedding_list_tmp)\n",
        "    return embedding_list"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qzmKa42yRrnX"
      },
      "source": [
        "# input features: pretrained word vectors of each token\n",
        "# return a list of feature dicts, each feature dict corresponding to each token\n",
        "def Feature(embed_list):\n",
        "    feature_list = list()\n",
        "    for idx_list in range(len(embed_list)):\n",
        "        feature_list_tmp = list()\n",
        "        for idx_tuple in range(len(embed_list[idx_list])):\n",
        "            feature_dict = dict()\n",
        "            for idx_vec in range(len(embed_list[idx_list][idx_tuple])):\n",
        "                feature_dict['dim_' + str(idx_vec+1)] = embed_list[idx_list][idx_tuple][idx_vec]\n",
        "            feature_list_tmp.append(feature_dict)\n",
        "        feature_list.append(feature_list_tmp)\n",
        "    return feature_list"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dNa4fSkYRrnX"
      },
      "source": [
        "# get the labels of each tokens in train.data\n",
        "# return a list of lists of labels\n",
        "def Preprocess(data_list):\n",
        "    label_list = list()\n",
        "    for idx_list in range(len(data_list)):\n",
        "        label_list_tmp = list()\n",
        "        for idx_tuple in range(len(data_list[idx_list])):\n",
        "            label_list_tmp.append(data_list[idx_list][idx_tuple][1])\n",
        "        label_list.append(label_list_tmp)\n",
        "    return label_list"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wdoz84b0RrnX"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e_uGdFy3RrnX"
      },
      "source": [
        "data_list, traindata_list, testdata_list, traindata_article_id_list, testdata_article_id_list = Dataset(data_path)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BDc_mtuXRrnX"
      },
      "source": [
        "# Load Word Embedding\n",
        "trainembed_list = Word2Vector(traindata_list, word_vecs)\n",
        "testembed_list = Word2Vector(testdata_list, word_vecs)\n",
        "\n",
        "# CRF - Train Data (Augmentation Data)\n",
        "x_train = Feature(trainembed_list)\n",
        "y_train = Preprocess(traindata_list)\n",
        "\n",
        "# CRF - Test Data (Golden Standard)\n",
        "x_test = Feature(testembed_list)\n",
        "y_test = Preprocess(testdata_list)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hbhes9eBRrnX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f09348ce-637b-41ff-fcc9-86305396eb76"
      },
      "source": [
        "y_pred, y_pred_mar, f1score = CRF(x_train, y_train, x_test, y_test)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "  B-location      0.000     0.000     0.000        15\n",
            "  I-location      0.000     0.000     0.000        41\n",
            "  B-med_exam      0.333     0.030     0.056        33\n",
            "  I-med_exam      1.000     0.075     0.140        80\n",
            "     B-money      0.500     0.333     0.400        12\n",
            "     I-money      0.375     0.171     0.235        35\n",
            "      B-name      1.000     0.143     0.250         7\n",
            "      I-name      0.333     0.100     0.154        10\n",
            "      B-time      0.648     0.414     0.505       111\n",
            "      I-time      0.844     0.468     0.602       265\n",
            "\n",
            "   micro avg      0.735     0.310     0.436       609\n",
            "   macro avg      0.503     0.174     0.234       609\n",
            "weighted avg      0.683     0.310     0.402       609\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "H02UitQBRrnX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad8bfb91-5052-47f5-db44-88507b6d74c8"
      },
      "source": [
        "f1score"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.4022074431917941"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j0nOItHSRrnX"
      },
      "source": [
        "## Output data\n",
        "* Change model output into `output.tsv` \n",
        "* Only accept this output format uploading to competition system"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QC66RB5tRrnX"
      },
      "source": [
        "output=\"article_id\\tstart_position\\tend_position\\tentity_text\\tentity_type\\n\"\n",
        "for test_id in range(len(y_pred)):\n",
        "    pos=0\n",
        "    start_pos=None\n",
        "    end_pos=None\n",
        "    entity_text=None\n",
        "    entity_type=None\n",
        "    for pred_id in range(len(y_pred[test_id])):\n",
        "        if y_pred[test_id][pred_id][0]=='B':\n",
        "            start_pos=pos\n",
        "            entity_type=y_pred[test_id][pred_id][2:]\n",
        "        elif start_pos is not None and y_pred[test_id][pred_id][0]=='I' and y_pred[test_id][pred_id+1][0]=='O':\n",
        "            end_pos=pos\n",
        "            entity_text=''.join([testdata_list[test_id][position][0] for position in range(start_pos,end_pos+1)])\n",
        "            line=str(testdata_article_id_list[test_id])+'\\t\\t'+str(start_pos)+'\\t\\t'+str(end_pos+1)+'\\t\\t'+entity_text+'\\t\\t'+entity_type\n",
        "            output+=line+'\\n'\n",
        "        pos+=1     "
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "40HnTWGtRrnX"
      },
      "source": [
        "output_path='/content/drive/MyDrive/Colab Notebooks/NER/output.tsv'\n",
        "with open(output_path,'w',encoding='utf-8') as f:\n",
        "    f.write(output)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "-M36pQhxRrnX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6de8c6d3-38a6-42d7-8e5a-145cce620388"
      },
      "source": [
        "print(output)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "article_id\tstart_position\tend_position\tentity_text\tentity_type\n",
            "8\t\t52\t\t54\t\t前天\t\ttime\n",
            "8\t\t189\t\t193\t\t二十分鐘\t\ttime\n",
            "8\t\t293\t\t295\t\t五年\t\ttime\n",
            "8\t\t540\t\t544\t\t兩個禮拜\t\ttime\n",
            "8\t\t726\t\t728\t\t前天\t\ttime\n",
            "8\t\t730\t\t732\t\t前天\t\ttime\n",
            "8\t\t858\t\t860\t\t前天\t\ttime\n",
            "8\t\t898\t\t900\t\t前天\t\ttime\n",
            "8\t\t1549\t\t1551\t\t五天\t\ttime\n",
            "8\t\t1622\t\t1626\t\t五天禮拜\t\ttime\n",
            "8\t\t2560\t\t2563\t\t兩個月\t\ttime\n",
            "16\t\t51\t\t55\t\t九、十點\t\ttime\n",
            "16\t\t60\t\t64\t\t九、十點\t\ttime\n",
            "16\t\t122\t\t124\t\t三年\t\ttime\n",
            "16\t\t130\t\t132\t\t三年\t\ttime\n",
            "16\t\t247\t\t249\t\t三年\t\ttime\n",
            "0\t\t1268\t\t1271\t\t8公分\t\tmed_exam\n",
            "0\t\t2576\t\t2578\t\t五天\t\ttime\n",
            "0\t\t2665\t\t2670\t\t二到禮拜四\t\ttime\n",
            "24\t\t48\t\t51\t\t三個月\t\ttime\n",
            "24\t\t53\t\t56\t\t七公斤\t\tmed_exam\n",
            "24\t\t113\t\t115\t\t三年\t\ttime\n",
            "24\t\t141\t\t143\t\t三年\t\ttime\n",
            "24\t\t1381\t\t1384\t\t一點點\t\ttime\n",
            "24\t\t1381\t\t1410\t\t一點點麻煩啦，不比這個……民眾：不能馬上拿啦。醫師：啊明明\t\ttime\n",
            "24\t\t1855\t\t1861\t\t九到十二個月\t\ttime\n",
            "24\t\t1866\t\t1872\t\t九到十二個月\t\ttime\n",
            "24\t\t1986\t\t1989\t\t五個月\t\ttime\n",
            "11\t\t18\t\t21\t\t九千七\t\ttime\n",
            "11\t\t61\t\t64\t\t零四百\t\tmoney\n",
            "11\t\t67\t\t70\t\t零五百\t\tmoney\n",
            "11\t\t83\t\t86\t\t九千七\t\ttime\n",
            "11\t\t135\t\t139\t\t三個禮拜\t\ttime\n",
            "11\t\t514\t\t518\t\t三個禮拜\t\ttime\n",
            "11\t\t557\t\t561\t\t三個禮拜\t\ttime\n",
            "11\t\t569\t\t573\t\t三個禮拜\t\ttime\n",
            "11\t\t645\t\t649\t\t三個禮拜\t\ttime\n",
            "11\t\t672\t\t676\t\t三個禮拜\t\ttime\n",
            "11\t\t693\t\t697\t\t兩個禮拜\t\ttime\n",
            "11\t\t707\t\t713\t\t後四月二十日\t\ttime\n",
            "11\t\t751\t\t755\t\t兩個禮拜\t\ttime\n",
            "11\t\t923\t\t925\t\t小櫻\t\tname\n",
            "9\t\t70\t\t74\t\t兩千多塊\t\tmoney\n",
            "9\t\t187\t\t190\t\t5公斤\t\tmed_exam\n",
            "9\t\t290\t\t294\t\t四十分鐘\t\ttime\n",
            "13\t\t394\t\t397\t\t六個月\t\ttime\n",
            "13\t\t550\t\t553\t\t六個月\t\ttime\n",
            "13\t\t678\t\t681\t\t5月初\t\ttime\n",
            "13\t\t701\t\t705\t\t兩個禮拜\t\ttime\n",
            "13\t\t721\t\t723\t\t半月\t\ttime\n",
            "13\t\t731\t\t735\t\t5月1號\t\ttime\n",
            "13\t\t784\t\t788\t\t四個禮拜\t\ttime\n",
            "13\t\t793\t\t797\t\t四個禮拜\t\ttime\n",
            "13\t\t946\t\t950\t\t四個禮拜\t\ttime\n",
            "1\t\t235\t\t237\t\t中午\t\ttime\n",
            "1\t\t254\t\t256\t\t中午\t\ttime\n",
            "1\t\t443\t\t446\t\t一點點\t\ttime\n",
            "1\t\t802\t\t805\t\t三個月\t\ttime\n",
            "1\t\t906\t\t908\t\t二百\t\tmoney\n",
            "1\t\t917\t\t919\t\t二百\t\tmoney\n",
            "1\t\t988\t\t990\t\t兩百\t\tmoney\n",
            "1\t\t997\t\t999\t\t兩百\t\tmoney\n",
            "1\t\t1021\t\t1023\t\t兩百\t\tmoney\n",
            "1\t\t1052\t\t1055\t\t五個月\t\ttime\n",
            "1\t\t1385\t\t1387\t\t六年\t\ttime\n",
            "1\t\t1394\t\t1396\t\t四年\t\ttime\n",
            "1\t\t1404\t\t1406\t\t四年\t\ttime\n",
            "1\t\t1414\t\t1416\t\t四年\t\ttime\n",
            "1\t\t1525\t\t1528\t\t一點點\t\ttime\n",
            "1\t\t1533\t\t1536\t\t一點點\t\ttime\n",
            "1\t\t1575\t\t1578\t\t一點點\t\ttime\n",
            "1\t\t1765\t\t1768\t\t一點點\t\ttime\n",
            "1\t\t2261\t\t2263\t\t半月\t\ttime\n",
            "1\t\t2295\t\t2299\t\t兩個禮拜\t\ttime\n",
            "1\t\t2334\t\t2338\t\t三個禮拜\t\ttime\n",
            "1\t\t2591\t\t2593\t\t爬樓\t\tlocation\n",
            "23\t\t275\t\t278\t\t三個月\t\ttime\n",
            "23\t\t535\t\t538\t\t三個月\t\ttime\n",
            "23\t\t540\t\t543\t\t二個月\t\ttime\n",
            "23\t\t556\t\t559\t\t三個月\t\ttime\n",
            "23\t\t629\t\t633\t\t8月1號\t\ttime\n",
            "23\t\t1592\t\t1596\t\t8月2號\t\ttime\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36oJuO06RrnX"
      },
      "source": [
        "## Note\n",
        "* You may try `python-crfsuite` to train an neural network for NER tagging optimized by gradient descent back propagation\n",
        "    * [Documentation](https://github.com/scrapinghub/python-crfsuite)\n",
        "* You may try `CRF++` tool for NER tagging by CRF model\n",
        "    * [Documentation](http://taku910.github.io/crfpp/)\n",
        "    * Need design feature template\n",
        "    * Can only computed in CPU\n",
        "* You may try other traditional chinese word embedding (ex. fasttext, bert, ...) for input features\n",
        "* You may try add other features for NER model, ex. POS-tag, word_length, word_position, ...\n",
        "* You should upload the prediction output on `development data` or `test data` provided later to the competition system. Note don't upload prediction output on the splitted testing dataset like this baseline example."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yOuPLXj8RrnX"
      },
      "source": [
        "-----------------------------------------------------"
      ]
    }
  ]
}